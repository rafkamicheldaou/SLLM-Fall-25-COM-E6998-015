{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "daba9265"
   },
   "source": [
    "\n",
    "# Hugging Face — Attention & Adapters Quickstart\n",
    "\n",
    "This notebook demonstrates **Hugging Face** examples for:\n",
    "- Enabling **FlashAttention-2** (or SDPA fallback) in `transformers`\n",
    "- Adding **LoRA** adapters with **PEFT** (LongLoRA-style LoRA+ notes)\n",
    "- Optional **bitsandbytes** quantization for memory savings\n",
    "- Minimal **SFT training loop** with PEFT on a tiny sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "e14f50d6"
   },
   "source": [
    "\n",
    "> **Default base model:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (≈1.1B params).  \n",
    "> Runs comfortably on a single **A100** in Colab, even without quantization.  \n",
    "> Alternatives you can try:\n",
    "> - `Qwen/Qwen2.5-1.5B-Instruct` (~1.5B)\n",
    "> - `microsoft/Phi-3-mini-4k-instruct` (~3.8B) — use 4-bit for comfort on smaller GPUs\n",
    ">\n",
    "> For larger models (3–7B), enable the **bitsandbytes 4-bit** cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce13f051",
    "outputId": "d006fdc4-80ba-429e-f9a6-ef977b81f775"
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U transformers accelerate peft datasets\n",
    "%pip install -U --index-url https://download.pytorch.org/whl/cu121 torch\n",
    "%pip install -U flash-attn       # optional; requires matching CUDA & PyTorch\n",
    "%pip install -U bitsandbytes     # optional for 8-bit / 4-bit loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "7640a4cf"
   },
   "source": [
    "\n",
    "## 1) Enable FlashAttention-2 (or SDPA) in `transformers`\n",
    "\n",
    "Use `attn_implementation=\"flash_attention_2\"` when `flash-attn` is installed. Otherwise, try `\"sdpa\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482,
     "referenced_widgets": [
      "498d97ec0a1445ffa4a53cb5f2eb4868",
      "c1aa203b879e41ecb8cb64018ec148db",
      "2946e7e97a4242d6aded47f329f77792",
      "bfc2e9c400f9454788503af4dcf3b8ef",
      "217c8d96c9124adcbbd3ffcbc8cce666",
      "f9484481e9754ef5b686ca4cc2bf439f",
      "a6075eba7eb84cb0b3fb7edab1414f9e",
      "702eff1364194728ac160596e3a511f5",
      "6616135a695a41c191862fe14434d5fc",
      "5cf1c4558eee4d7eaf908c7684b1a60e",
      "bf25d7c17ef34d8ba093eb57deec1d16",
      "1e4af0b1f9c0448baeb83c980631f90e",
      "7b00fce05bd14d16aaf3894c23a13602",
      "0ee2c594226c450c943d637b9d864ce7",
      "8d619cf7297044ce99c75714549569bb",
      "c9df39810a8648fc9e60e61490ef2705",
      "d116a7a5773148b39f327f7e733be010",
      "55de1c19eb8a47aaa74144f7322b6120",
      "8ce0f71633cb40cb83c90ce9a029a7fd",
      "ca62ac70fe2c43e5aafbb372f233452c",
      "0e6a399b8b9146e6b51fab278189c6be",
      "d570ff80996b4e07ac04ea446b8d19be",
      "f6be2d4194c547558d0eee64905b66b4",
      "50d050df6cc24d83b095a0635e6a417b",
      "590648fb3fd34481b142bbf9cab09990",
      "1f7153a760884eba89f822d25432766e",
      "20977727414b4590a027a7627250ee74",
      "3487189f60d647d1b81016069c793c1d",
      "b448a2ed4ab84f07b329c68a37d8a6a4",
      "b65f47681471492294b75d2ee25dde19",
      "cea534229e0c4353ab10e1a2232497f3",
      "7352e215276a47948faabd3a63eb0f5f",
      "e2a36738d8104a799e17bc78e33770e8",
      "3ee6d241b62b4779a2896d6b3dadec57",
      "8b4b639f84014fa8bf61d6c4ffd0e72f",
      "417236ff861c4216b96241ca4978f78d",
      "8e65835d91fc4e82a85a1b692ca77aee",
      "d3cebe7618314cb9972fbae00e07ee69",
      "78d05d159c6f45fbbd8fc986673d326a",
      "8a3be9708bbe4cd79f68b97264d66c55",
      "54a64a9392c54669882c9125b0910e33",
      "1fa653070b8c4838ac3e6091599841ef",
      "99ccd81019214cc998752dfee10e57fa",
      "f2ff8f5ae4994d1c8c71fc8ba9bafb88",
      "f7ee491cb1bc4bb8be3308588577ded8",
      "338c886e156f4fbaa9dcd83f9fed310e",
      "5abe03d6cfb5431da9a8284bd7536bf8",
      "5355f16b0b3f408790bf164dfd712d80",
      "f6855feecce847b1b6236c8761327641",
      "328823c740104b8bbcda9834746e8dda",
      "167bf521b99e4c6da4d765c9b98d42ca",
      "6169071d922340beafc1abb0c2c259bb",
      "90523677971d4bc8a7eb9be1df4e73aa",
      "f47945ff503d4047b7a71e5543825f08",
      "9ac3254e00f540bc818d8e11ebcf09b3",
      "46367f2002a6435fa0865188bcc6aaa1",
      "fed7585db8b3428283dce7cc5a129c87",
      "5ffc1aa2eed1445382324c334c91b599",
      "03b0b0b540f44dd7bdd4f050423c20d7",
      "79953d6074ed459fbf3977d000cc98c1",
      "c31fd2ebe7c5479f866d87e0cc256e1f",
      "b8afe5b4648545cc9aa07707e81dfaf9",
      "232caf5a2be54bb4ba0bfc5a554701d7",
      "ac3fb42628514b83a5861e08cc032971",
      "da51c6f53a2e4b63a993a341e21ddea7",
      "40b02d49a60c45bdbfab410fe3396196",
      "a467fd4bd8f648aa955a23ac9b23e789",
      "34c19325ff1d461091c85ae45959a8f4",
      "b7932dd0a2674e55a2c4e3bab319e37b",
      "17c538866d924ed18709eb36013c777c",
      "c2cbc88f760f47aca8f0282bd55cd633",
      "857202fab7624a83af759d324d0b6d84",
      "ae9f25314dec402c831337c9df730cb0",
      "fd9a6f787ff542f7a481162622d40f9e",
      "47613658a6dd4d18882988f4bcd4bd5e",
      "38393784c8c4466e9f31af16b817ea75",
      "5e7f3b0030ec49cdba5100c4c4d2eb24"
     ]
    },
    "id": "6a4f249d",
    "outputId": "927b01b9-cd2e-410f-915d-5b749442f376"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # replace with a larger long-context model when you have a GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Try FlashAttention-2 first; fall back to SDPA if unavailable.\n",
    "attn_backend = \"flash_attention_2\"\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        attn_implementation=attn_backend,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"flash_attention_2 not available, falling back to sdpa. Err:\", e)\n",
    "    attn_backend = \"sdpa\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        attn_implementation=attn_backend,\n",
    "    )\n",
    "\n",
    "print(\"Using attention backend:\", attn_backend)\n",
    "\n",
    "inputs = tokenizer(\"Explain FlashAttention-2 briefly.\", return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "cac02573"
   },
   "source": [
    "\n",
    "## 2) Optional: 4-bit / 8-bit loading with **bitsandbytes**\n",
    "\n",
    "This helps fit larger models in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "3ce1dcaa"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "#     attn_implementation=\"sdpa\",   # or \"flash_attention_2\" if flash-attn is installed\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "cc771c4d"
   },
   "source": [
    "\n",
    "## 3) Add **LoRA** Adapters with **PEFT**\n",
    "\n",
    "This mirrors LongLoRA's LoRA idea. For **LoRA+** (from the paper), also set **embeddings** and **norms** trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9598ff12",
    "outputId": "160c5141-8c2b-4da4-8d4b-09544c5ec59c"
   },
   "outputs": [],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Reuse `model` from above (or load a larger base).\n",
    "# Choose target modules based on the model architecture; adjust names for LLaMA-style models (q_proj, k_proj, etc.).\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # adjust per model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_cfg)\n",
    "print(\"LoRA attached. Trainable params:\", sum(p.numel() for p in peft_model.parameters() if p.requires_grad))\n",
    "\n",
    "# LongLoRA-style tweak (LoRA+): also train embeddings & norms\n",
    "for name, p in peft_model.named_parameters():\n",
    "    if any(k in name for k in [\"embed_tokens\", \"wte\", \"ln_\", \"norm\"]):\n",
    "        p.requires_grad = True\n",
    "\n",
    "print(\"Trainable after LoRA+ tweak:\", sum(p.numel() for p in peft_model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "4403ec17"
   },
   "source": [
    "\n",
    "## 4) Minimal SFT Training Loop (Toy Example)\n",
    "\n",
    "A tiny example to show the **PEFT** workflow. Replace the dataset with your class data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57cdf521",
    "outputId": "6d3ec9c5-ed6c-4c08-b2d5-fd0f674fd08b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class TinyTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts):\n",
    "        self.data = []\n",
    "        for t in texts:\n",
    "            ids = tokenizer(\n",
    "                t,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=64\n",
    "            )\n",
    "            # Remove the extra leading batch dimension\n",
    "            ids = {k: v.squeeze(0) for k, v in ids.items()}\n",
    "            ids[\"labels\"] = ids[\"input_ids\"].clone()\n",
    "            self.data.append(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "texts = [\n",
    "    \"Summarize: FlashAttention-2 reduces memory I/O using tiling and online softmax.\",\n",
    "    \"Explain: Shifted sparse attention groups tokens and shifts half heads for info flow.\",\n",
    "    \"Describe: LoRA updates a small low-rank set of weights for efficient fine-tuning.\",\n",
    "]\n",
    "\n",
    "ds = TinyTextDataset(tokenizer, texts)\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "device = next(peft_model.parameters()).device\n",
    "optim = torch.optim.AdamW([p for p in peft_model.parameters() if p.requires_grad], lr=2e-4)\n",
    "\n",
    "peft_model.train()\n",
    "for step, batch in enumerate(dl):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    out = peft_model(**batch)\n",
    "    loss = out.loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    print(f\"step {step} loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "6995da0e"
   },
   "source": [
    "\n",
    "## 5) Inference with the LoRA Adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f66819",
    "outputId": "85055fc1-1ec8-4ee0-94f0-8c71df722a9c"
   },
   "outputs": [],
   "source": [
    "\n",
    "peft_model.eval()\n",
    "prompt = \"What is one benefit of FlashAttention-2?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    gen = peft_model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(gen[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
